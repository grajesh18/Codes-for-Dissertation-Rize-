{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa888588-d0a0-497f-9b82-244a75381507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gautham Rajesh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "**Message:**\n",
      "\n",
      "\"Remember, pain is a normal part of life. Instead of avoiding it, try to make space for it. This week, notice when uncomfortable emotions arise and practice accepting them without judgment. Reflect on how you can respond in a way that aligns with your values, even in the presence of discomfort. You're building a meaningful life, one step at a time.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setting up my OpenAI client\n",
    "OPENAI_API_KEY = \"Use you API Key\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Loading the notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and building a FAISS index for notifications\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Reading the session summary and partial transcript\n",
    "with open(\"Chat Gpt S1 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session One.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting the transcript to 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant candidate messages based on session summary\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Building system and user prompts for message selection\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT (Acceptance and Commitment Therapy) principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one.\n",
    "It should be:\n",
    "- Short and supportive\n",
    "- Actionable (something the client can reflect on or practice)\n",
    "- Grounded in ACT processes (defusion, acceptance, values, committed action)\n",
    "\"\"\"\n",
    "\n",
    "# Asking the model for the best between-session message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1452bff9-4c35-4618-ba05-b0021f894a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "<think>\n",
      "Alright, let me process this request carefully. The client is in a delicate place - they're intellectually skeptical about the therapy process but willing to engage tentatively. The core tension is between their need for logical understanding (\"if you can't describe it, you don't understand it\") and the therapist's invitation to embrace experiential learning (\"progress comes first—then understanding\").\n",
      "\n",
      "The three candidate messages each target different ACT processes:\n",
      "1. Self-as-context focuses on the observing self\n",
      "2. Present-moment targets mindfulness\n",
      "3. Values connects to motivation\n",
      "\n",
      "Considering the session dynamics, the client's stated discomfort stems from cognitive fusion (over-analyzing the method) and experiential avoidance (disliking the \"worried\" feeling). A values-based message would be most potent here because:\n",
      "- It validates their courage in proceeding despite discomfort\n",
      "- Shifts focus from intellectualizing to purposeful action\n",
      "- Aligns with their expressed willingness to \"go on faith for a while\"\n",
      "\n",
      "The rewritten message should:\n",
      "- Acknowledge their discomfort without reinforcing struggle\n",
      "- Connect to their stated value of growth/personal development\n",
      "- Offer a concrete, small-scale practice\n",
      "- Maintain the ACT principle of workability over correctness\n",
      "\n",
      "The skiing metaphor from session is particularly useful - we can extend that kinesthetic imagery to help ground the suggestion in their lived experience.\n",
      "</think>\n",
      "\n",
      "\n",
      "Based on the client's willingness to proceed despite discomfort and their need to shift from intellectualizing to experiential engagement, the optimal message is:\n",
      "\n",
      "**Rewritten ACT-aligned message:**  \n",
      "*\"When uncertainty arises, gently acknowledge your thoughts like passing clouds—then reconnect with why you chose this path. What small step could you take today that aligns with your courage to grow?\"*\n",
      "\n",
      "**Why this fits:**  \n",
      "1. **Targets Fusion & Avoidance:** Addresses their \"worry\" by normalizing uncertainty through the \"passing clouds\" metaphor (defusion).  \n",
      "2. **Honors Values:** Directly connects to their demonstrated value of courage (\"willing to go on faith\") and growth.  \n",
      "3. **Action-Oriented:** Focuses on a \"small step\" to bypass over-analysis, echoing the skiing metaphor’s emphasis on movement over explanation.  \n",
      "4. **Builds Acceptance:** Validates discomfort while orienting toward chosen commitment.  \n",
      "\n",
      "*Alternative candidate assessment:*  \n",
      "- Self-as-context: Too abstract for current skepticism.  \n",
      "- Present-moment: Useful but misses their core struggle with purpose.  \n",
      "- Values: Strong, but the rewrite integrates it more fluidly with their immediate experience.  \n",
      "\n",
      "This message supports the therapeutic direction of prioritizing experiential engagement while gently reinforcing their willingness.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and DeepSeek model\n",
    "HF_TOKEN = \"Use you API Key\"\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-0528:novita\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI compatibility\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notifications\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving the index and notification metadata for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading the session summary and transcript\n",
    "with open(\"Deepseek S1 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session One.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Creating system and user prompts with session and candidate messages\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the DeepSeek model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Displaying the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5acd7eb2-d636-42e5-892d-6eb4296b0d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "Based on the session summary and the client's concerns, I would select the message:\n",
      "\n",
      "\"There's a part of you that observes experiences without judgment—that self is always available, no matter what thoughts arise.\"\n",
      "\n",
      "This message is short, actionable, and grounded in ACT principles. It acknowledges the client's tendency to overthink and analyze, and gently reminds them that they have a part of themselves that can observe their experiences without judgment. This message is particularly relevant because the client has expressed a willingness to trust the therapist and take a leap of faith, and this message can help them cultivate a sense of self-awareness and acceptance.\n",
      "\n",
      "This message is also consistent with the therapist's approach, which emphasizes the importance of being in the present moment and taking action towards goals, rather than getting bogged down in theory and analysis. By reminding the client of their observing self, the message encourages them to stay grounded and focused on the present moment, rather than getting caught up in worries about the therapy process or their own thoughts and feelings.\n",
      "\n",
      "Overall, this message is a gentle and supportive reminder that can help the client stay on track and build their confidence in the therapy process.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and Meta Llama model\n",
    "HF_TOKEN = \"Use you API Key\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct:cerebras\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notification messages\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving index and notification chunks for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading session summary and transcript files\n",
    "with open(\"Meta S1 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session One.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS search\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Constructing system and user prompts for the model\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the Meta Llama model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb2c301f-d9ec-4f0a-9b88-b9b58d5ffd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "“Between now and our next meeting, notice one moment when the mind demands a map before you’ll move. Instead of solving the uncertainty, practice taking one small step while the worry is still talking—like skiing before the instructions feel complete. Let me know what you discover.”\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and Moonshot model\n",
    "HF_TOKEN = \"Use you API Key\"\n",
    "MODEL_ID = \"moonshotai/Kimi-K2-Instruct:novita\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notification messages\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving index and notification chunks for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading session summary and transcript files\n",
    "with open(\"Moonshot S1 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session One.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS search\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Constructing system and user prompts for the model\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the Moonshot model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d0662a-baba-453b-9502-09963d0d2783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
