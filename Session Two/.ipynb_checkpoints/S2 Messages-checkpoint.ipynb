{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa888588-d0a0-497f-9b82-244a75381507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gautham Rajesh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "**Message:**\n",
      "\"Remember, it's okay to feel pain and frustration; they are part of being human. Instead of trying to control these feelings, try to make space for them. When they arise, observe them as if you’re watching clouds pass by in the sky. This practice of acceptance can help you respond in ways that align with your values and lead to a more meaningful life. You have the strength to navigate through this.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setting up my OpenAI client\n",
    "OPENAI_API_KEY = \"Your Open API Key\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Loading the notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and building a FAISS index for notifications\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Reading the session summary and partial transcript\n",
    "with open(\"Chat Gpt S2 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Two.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting the transcript to 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant candidate messages based on session summary\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Building system and user prompts for message selection\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT (Acceptance and Commitment Therapy) principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one.\n",
    "It should be:\n",
    "- Short and supportive\n",
    "- Actionable (something the client can reflect on or practice)\n",
    "- Grounded in ACT processes (defusion, acceptance, values, committed action)\n",
    "\"\"\"\n",
    "\n",
    "# Asking the model for the best between-session message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1452bff9-4c35-4618-ba05-b0021f894a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gautham Rajesh\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "<think>\n",
      "Hmm, the user is asking me to select or craft a therapeutic message for a client based on ACT principles, following a session summary that highlights avoidance patterns. The client clearly uses suppression (\"emotional damper\") and delayed processing to manage anger flares, acknowledging this strategy hasn't worked long-term despite short-term relief.\n",
      "\n",
      "Looking at the candidate messages:\n",
      "- The avoidance-focused message accurately names the pattern but feels too abstract. It states a truth but lacks actionable guidance.\n",
      "- The self-as-context option points to an important ACT concept but might feel overly abstract to a client currently fused with their \"emotional damper\" metaphor.\n",
      "- The acceptance message comes closest by naming pain normalization but could more directly address the client's specific suppression behavior.\n",
      "\n",
      "The core opportunity here is to validate the client's experience while gently challenging the workability of their avoidance strategy. A better message would:\n",
      "1. Name their specific pattern (\"emotional damper\")\n",
      "2. Acknowledge its short-term function\n",
      "3. Pose the workability question that emerged in session\n",
      "4. Suggest a small, concrete alternative (mindful noticing)\n",
      "\n",
      "Rewriting allows me to incorporate the client's own language (\"emotional damper\") which builds rapport, while making the workability question - already resonating with them - the centerpiece. Ending with a micro-action (noticing without changing) aligns with ACT's focus on present-moment awareness as an alternative to avoidance.\n",
      "\n",
      "The rewritten message also stays within ACT's six core processes: it touches on acceptance (allowing discomfort), defusion (observing thoughts/feelings), and workability (evaluating strategies), while being concise enough for between-session reflection.\n",
      "</think>\n",
      "\n",
      "\n",
      "Based on the session summary, the **best message** would be a rewritten version that directly addresses the client's avoidance pattern while incorporating ACT principles and their own language:\n",
      "\n",
      "**Rewritten Message:**  \n",
      "*\"When anger flares, you've skillfully used the 'emotional damper' for quick relief. That makes sense in the moment. But what if, just once this week, you paused and simply noticed the anger without turning the damper on? Not to change it—just to observe what it feels like in your body and what thoughts it carries. We can explore what happens when you make space for it instead.\"*\n",
      "\n",
      "**Why this works:**  \n",
      "1.  **Validates & Names the Pattern:** Acknowledges their established coping strategy (\"emotional damper\") and its short-term function (\"quick relief\"), building rapport.  \n",
      "2.  **Highlights Workability:** Implicitly questions the long-term cost of avoidance (\"But what if...\") without judgment, aligning with the session's focus.  \n",
      "3.  **Offers Small, Actionable Step:** Proposes a concrete, manageable experiment in **acceptance** and **defusion** (\"pause... simply noticed... observe\").  \n",
      "4.  **Uses Client's Language:** Incorporates their metaphor (\"emotional damper\") and concept (\"make space\").  \n",
      "5.  **Focuses on Willingness:** Encourages curiosity (\"what it feels like... what thoughts\") instead of suppression.  \n",
      "6.  **\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and DeepSeek model\n",
    "HF_TOKEN = \"Your Open API Key\"\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-0528:novita\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI compatibility\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notifications\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving the index and notification metadata for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading the session summary and transcript\n",
    "with open(\"Deepseek S2 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Two.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Creating system and user prompts with session and candidate messages\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the DeepSeek model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Displaying the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5acd7eb2-d636-42e5-892d-6eb4296b0d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "Based on the session summary, I would choose the following message:\n",
      "\n",
      "\"Pain is a normal part of life. Rather than avoiding or fighting it, we can make space for it and choose how to respond.\"\n",
      "\n",
      "This message is short, actionable, and grounded in ACT principles. It acknowledges the client's experience of pain and discomfort, while encouraging them to approach it with a sense of acceptance and choice. By making space for their emotions, the client can begin to develop a greater sense of workability and fulfillment in their life, rather than just trying to avoid or suppress their negative emotions.\n",
      "\n",
      "This message also resonates with the client's statement about putting on an \"emotional damper\" and dealing with their emotions later, which is an avoidance strategy that has not been effective in the long term. By encouraging the client to make space for their emotions, we can help them develop a more direct and honest relationship with their experiences, which is a key aspect of ACT.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and Meta Llama model\n",
    "HF_TOKEN = \"Your Open API Key\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct:cerebras\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notification messages\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving index and notification chunks for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading session summary and transcript files\n",
    "with open(\"Meta S2 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Two.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS search\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Constructing system and user prompts for the model\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the Meta Llama model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2c301f-d9ec-4f0a-9b88-b9b58d5ffd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "Chosen message (slightly rewritten):\n",
      "\n",
      "“Next time the anger flares, try a 30-second experiment: instead of reaching for the emotional damper, simply notice the surge—thoughts, heat, tension—like weather passing across the sky of awareness. Ask, ‘If this feeling could speak, what value is it pointing me toward?’ Then decide, with that value in view, what tiny action you’d like to take next.”\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and Moonshot model\n",
    "HF_TOKEN = \"Your Open API Key\"\n",
    "MODEL_ID = \"moonshotai/Kimi-K2-Instruct:novita\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notification messages\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving index and notification chunks for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading session summary and transcript files\n",
    "with open(\"Moonshot S2 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Two.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS search\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Constructing system and user prompts for the model\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the Moonshot model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c436d-7f6a-49d3-a1f5-0de858c78813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
