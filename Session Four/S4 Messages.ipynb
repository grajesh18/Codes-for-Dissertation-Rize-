{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa888588-d0a0-497f-9b82-244a75381507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "**Message:**\n",
      "\n",
      "\"Remember, there’s a part of you that can observe your experiences without judgment. This 'neutral you' is always present, even when emotions feel overwhelming. Try to take a moment today to notice your thoughts and feelings as if you’re watching leaves float by on a stream. This practice can help you create space and choose how to respond, rather than getting caught up in the turbulence. You’re not alone in this journey.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setting up my OpenAI client\n",
    "OPENAI_API_KEY = \"Your API Key\"\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Loading the notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and building a FAISS index for notifications\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Reading the session summary and partial transcript\n",
    "with open(\"Chat Gpt S4 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Four.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting the transcript to 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant candidate messages based on session summary\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Building system and user prompts for message selection\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT (Acceptance and Commitment Therapy) principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one.\n",
    "It should be:\n",
    "- Short and supportive\n",
    "- Actionable (something the client can reflect on or practice)\n",
    "- Grounded in ACT processes (defusion, acceptance, values, committed action)\n",
    "\"\"\"\n",
    "\n",
    "# Asking the model for the best between-session message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1452bff9-4c35-4618-ba05-b0021f894a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "<think>\n",
      "Alright, let's tackle this therapy message selection. The client is clearly struggling with physical pain, emotional volatility, and self-critical thoughts after a medication change. They're frustrated about activity limitations (especially hiking) and even questioning therapy's value. \n",
      "\n",
      "The therapist noted the client's \"slippery\" mind that \"beats you up\" - classic fusion where thoughts dominate experience. The client themselves identified black-and-white thinking patterns. \n",
      "\n",
      "Looking at the message options:\n",
      "The acceptance message about pain being normal feels too generic for this client's acute distress. The avoidance warning is accurate but might feel accusatory when they're already self-critical. \n",
      "\n",
      "The self-as-context message resonates strongest. It directly addresses the \"you\" the client mentioned being intrigued by in session - that observing self untouched by pain or critical thoughts. This aligns with the therapist's goal to \"work around\" the dominant mind rather than fight it. \n",
      "\n",
      "But we can make it more actionable. Instead of just describing the concept, let's invite them to practice noticing that observing self during pain spikes. Adding \"when pain flares up\" makes it timely, and \"see if you can notice\" turns it into a concrete experiment matching their curiosity about this \"neutral 'you'\".\n",
      "\n",
      "The rewritten message keeps it ACT-core: connects to session content, targets defusion from the \"beating up\" mind, and is brief enough for between-session reflection.\n",
      "</think>\n",
      "\n",
      "\n",
      "Based on the client's frustration with physical pain, self-critical thoughts (\"mind that beats you up\"), and expressed curiosity about the observing self (\"intrigued by who that 'you' is\"), the most resonant message would be:\n",
      "\n",
      "**Best Choice:** [self-as-context]\n",
      "*\"There’s a part of you that observes experiences without judgment—that self is always available, no matter what thoughts arise.\"*\n",
      "\n",
      "**Why this fits best:**\n",
      "1.  **Directly addresses session content:** The client explicitly mentioned being intrigued by this concept (\"I'm real intrigued by who that 'you' is\").\n",
      "2.  **Targets core struggle:** It offers an alternative to the dominant, self-critical mind identified by the therapist as the core issue (\"very strong mind that beats you up\").\n",
      "3.  **Promotes defusion:** Helps create distance from painful thoughts and physical sensations by anchoring in the observing self.\n",
      "4.  **Validates without pushing:** It's a gentle reminder of an internal resource *already* noticed, not a directive to change feeling or thinking.\n",
      "5.  **Fosters curiosity:** Matches the client's expressed interest in exploring this part of themselves.\n",
      "\n",
      "**Slightly Enhanced Version (More Actionable):**\n",
      "> *\"When pain flares up or your mind gets loud, see if you can notice that part of you that's simply aware of it all - the 'you' that observes without needing to fight or fix. That space is\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and DeepSeek model\n",
    "HF_TOKEN = \"Your API Key\"\n",
    "MODEL_ID = \"deepseek-ai/DeepSeek-R1-0528:novita\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI compatibility\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notifications\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving the index and notification metadata for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading the session summary and transcript\n",
    "with open(\"Deepseek S4 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Four.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Creating system and user prompts with session and candidate messages\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the DeepSeek model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Displaying the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5acd7eb2-d636-42e5-892d-6eb4296b0d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "Based on the client's emotional state and the themes discussed in the session, I would choose the following message:\n",
      "\n",
      "\"There's a part of you that observes experiences without judgment—that self is always available, no matter what thoughts arise.\"\n",
      "\n",
      "This message is grounded in ACT principles, specifically the concept of self-as-context. It acknowledges the client's emotional turmoil and physical discomfort, while also offering a sense of stability and continuity. By emphasizing the presence of a non-judgmental observer, the message encourages the client to cultivate a sense of detachment from their thoughts and emotions, which can help reduce their sense of overwhelm and desperation.\n",
      "\n",
      "This message is also short, actionable, and easy to understand, making it a good fit for a client who may be feeling overwhelmed or struggling to focus. By incorporating the concept of self-as-context, the message offers a sense of hope and possibility, suggesting that the client has the capacity to observe their experiences without being controlled by them.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and Meta Llama model\n",
    "HF_TOKEN = \"Your API Key\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.1-8B-Instruct:cerebras\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notification messages\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving index and notification chunks for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading session summary and transcript files\n",
    "with open(\"Meta S4 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Four.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS search\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Constructing system and user prompts for the model\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the Meta Llama model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2c301f-d9ec-4f0a-9b88-b9b58d5ffd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 notification messages.\n",
      "Saved notification index.\n",
      "\n",
      " Suggested Between-Session Message:\n",
      "\n",
      "Between sessions, try this:  \n",
      "When pain or the “pinball” mind flares, pause for one slow breath and ask, “Right now, what tiny move—still or active—lets me stand for the parent, learner, and hiker I want to be?”  \n",
      "Then do that move, even if it’s just resting your foot or noticing one color on the wall.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setting up Hugging Face API and Moonshot model\n",
    "HF_TOKEN = \"Your API Key\"\n",
    "MODEL_ID = \"moonshotai/Kimi-K2-Instruct:novita\"\n",
    "\n",
    "# Initializing Hugging Face client with OpenAI-compatible API\n",
    "client = OpenAI(\n",
    "    base_url=\"https://router.huggingface.co/v1\",\n",
    "    api_key=HF_TOKEN,\n",
    ")\n",
    "\n",
    "# Loading all notification messages from file\n",
    "def load_notifications(filepath=\"notifications.txt\"):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return [chunk.strip() for chunk in text.strip().split(\"\\n\\n\") if chunk.strip()]\n",
    "\n",
    "notification_chunks = load_notifications(\"notifications.txt\")\n",
    "print(f\"Loaded {len(notification_chunks)} notification messages.\")\n",
    "\n",
    "# Creating embeddings and FAISS index for the notification messages\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "notification_embeddings = embedding_model.encode(notification_chunks, convert_to_numpy=True)\n",
    "\n",
    "dimension = notification_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(notification_embeddings.astype(\"float32\"))\n",
    "\n",
    "# Saving index and notification chunks for later use\n",
    "with open(\"notification_chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(notification_chunks, f, indent=2)\n",
    "faiss.write_index(index, \"notification_index.faiss\")\n",
    "\n",
    "print(\"Saved notification index.\")\n",
    "\n",
    "# Loading session summary and transcript files\n",
    "with open(\"Moonshot S4 Bottom Up Summary.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_summary = f.read().strip()\n",
    "\n",
    "with open(\"Session Four.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    session_transcript = f.read().strip()\n",
    "\n",
    "# Limiting transcript length to about 700 words for processing\n",
    "session_transcript = ' '.join(session_transcript.split()[:700])\n",
    "\n",
    "# Retrieving top-k relevant notification messages using FAISS search\n",
    "query_embedding = embedding_model.encode([session_summary], convert_to_numpy=True)\n",
    "D, I = index.search(np.array(query_embedding).astype(\"float32\"), k=3)\n",
    "top_messages = [notification_chunks[i] for i in I[0]]\n",
    "\n",
    "# Constructing system and user prompts for the model\n",
    "system_prompt = \"You are a psychologist selecting a helpful message to send to a client between sessions. Use ACT principles.\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Session Summary:\n",
    "---\n",
    "{session_summary}\n",
    "\n",
    "Transcript (partial):\n",
    "---\n",
    "{session_transcript}\n",
    "\n",
    "Candidate Messages:\n",
    "---\n",
    "{chr(10).join(top_messages)}\n",
    "\n",
    "Task:\n",
    "Choose the best message for this client or rewrite a better one. It should be short, actionable, and grounded in ACT.\n",
    "\"\"\"\n",
    "\n",
    "# Sending the request to the Moonshot model for message suggestion\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ],\n",
    "    temperature=0.4,\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "# Printing the suggested between-session message\n",
    "suggested_message = response.choices[0].message.content.strip()\n",
    "print(\"\\nSuggested Between-Session Message:\\n\")\n",
    "print(suggested_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b5095a-0f93-4d02-97c7-e4d0349426c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
